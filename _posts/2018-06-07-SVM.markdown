---
layout: post
title: Support Vector Machine
date: 2018-06-07
description: # add
img:  # Add image post (optional)
tags: [SVM,Maximum Margin]
---
SVM is one type of classification and regression algorithm  which is maily focused on the maximal margin hyperplane. Figure below shows two different classes seperating by three hyperplanes and also we can plot infinity hyperplanes to classify those two classes.  
![svm margin]({{site.baseurl}}/assets/img/svm.jpg)  
Although their training errors are zero there is no guarantee that the hyperplanes will perform equally well on unseen data.The classifier must choose one of these hyperplanes to represent its decision boundary, based on how well they are expected to perform on test samples.
Decision boundaries with large margins tend to have better generalization
errors than those with small margins.Intuitively, if the margin is small, then any slight perturbations to the decision boundary can have quite a significant impact on its classification Classifiers that produce decision boundaries with small margins are therefore more susceptible to model overfitting and tend to generalize poorly on previously unseen data, so SVM tries to locate the hyperplane such that distance is maximum from both the classes.  
##### How SVM is selecting margin: 
Consider a binary classification problem as shown in below figure and the hyperplane is \\(\pi\\). and two planes whcich are parallel to the hyperplane and passing through nearest point of each class are  \\(\pi_+\\) and  \\(\pi_-\\) as shown in below fig.  
![svm explanation]({{site.baseurl}}/assets/img/max_margin.jpg)  
Maximizing margin means maximizing distance between the \\(\pi_+\\) and  \\(\pi_-\\).so  our obkective is to maximize the distance between those two planes. Let's assume for all positive points y = +1 and for all negative points y = -1 so \\(y_i(W^Tx+b) \ge 1\\) for all points.To compute the margin, let x1 be a data point located on \\(\pi_+\\) and x2 be a data point on \\(\pi_-\\).
\\[\pi_+ is W^Tx_1 + b = +1\\]
\\[\pi_- is W^Tx_2 + b = -1\\]
by subtracting
\\[W.(x_1-x_2) = 2\\]
\\[||W|| * d = 2\\]
\\[d = \frac{2}{||W||}\\]
so our optimization problem is
\\[ Max \quad \frac{2}{||W||} \quad { S.T. } \quad y_i(W^Tx_i+b) \ge 1 \quad \forall \; i\\]
Max x = Min 1/x so we can write our problem as  
\\[ \text{Min } \quad \frac{||W||^2}{2} \quad { S.T } \quad y_i(W^Tx_i+b) \ge 1 \quad \forall \; i\\]
This SVM formulation presented in the previous section constructs only decision boundaries that are mistake-free. so for non separable data all constraints wont satisfy. This is called Hard margin SVM. so The learning algorithm in SVM must consider the trade-off between
the width of the margin and the number of training errors committed by the decision boundary. For this introducing positive-valued slack variables \\(\xi\\) into the constraints of the optimization such that for all positive points in positive side \\(\xi_i = 0\\), for all negative points in negative side \\(\xi_i = 0\\), for all positive points in negative side \\(\xi_i > 0\\) and \\(\xi_i\\) is distance from \\(\pi_+\\), for all negative points in positive side \(\xi_i > 0\\) and \\(\xi_i\\) is distance from \\(\pi_-\\). \\(\xi_i\\) is high for the points far away from correct hyperplane  \\(\pi_+\\) or \\(\pi_-\\).  
So our new formulation is 
\\[( W^* , b^* ) = \text{ arg min }\; \frac{||W||^2}{2}+C(\frac{1}{n}\sum_{i=1}^n\xi_i) \quad { S.T }\quad y_i(W^Tx_i+b) \ge 1-\xi_i \; \forall i \; \text{ and } \; \xi_i > 0\\]
We can observe that, above formulation can be assumed like \\(C(\frac{1}{n}\sum_{i=1}^n\xi_i)\\) as Loss and \\(\frac{||W||^2}{2}\\) as regularization term. \\(\xi_i\\) is the distance from correct hyperplane so \\(\xi_i\\) = 1 + absolute distance from decision hyperplane \\(\pi\\) for incorrectly classified points and 0  for correctly classified points.
\\[ \xi_i = \begin{cases}
1 \; + abs(W^Tx_i+b) \quad \text{ for incorrectly classified points } \newline 
0 \quad \text{ for correctly classified points } \newline 
\end{cases}\\]
for incorrectly classified points the absolute distance can be written as \\(-y_i(W^Tx_i+b)\\),  because \\(W^Tx_i+b\\) gives signed distance and and \\(y_i\\) is +1 for positive class and -1 for negative class. so for positive class points in negative region \\(W^Tx_i+b\\) is negatative,\\(y_i\\) is positive so  \\(-y_i(W^Tx_i+b)\\) is positive. for negative class points in positive region \\(W^Tx_i+b\\) is positive,\\(y_i\\) is negative so  \\(-y_i(W^Tx_i+b)\\) is positive.
\\[ \xi_i = \begin{cases}
1 \; - y_i(W^Tx_i+b) \quad \text{ for incorrectly classified points } \newline 
0 \quad \text{ for correctly classified points } \newline 
\end{cases}\\]
also for correctly classified points \\(-y_i(W^Tx_i+b)\\) is negative so we can write as
\\[ \xi_i =  MAX(0\;,1 \; - y_i(W^Tx_i+b))\\]
final optimization problem is 
\\[ Min \; C(\sum_{i=1}^n \; MAX(0\;,1 \; - y_i(W^Tx_i+b))) \; + \; \frac{||W||^2}{2}\\]
This is nothing but a [Hing Loss](https://en.wikipedia.org/wiki/Hinge_loss).
Where C is the hyperparameter, if C increses ten frequency to make mistakes will be decresed,this may led to overfitting to data. If C decreses frequency to make mistakes increases, this may lead to underfit.If C=0, then this is hard margin SVM.  
##### Solution for Optimization With out slack varible:  
our optimization problem is to minimize \\(\frac{||W||^2}{2}\\) such that \\(y_i(W^Tx_i+b) \ge 1 \; \forall i = 1,2..n\\). so Lagrangian is (for optimization tutorial you can see [here](https://udibhaskar.github.io/ml_blog/Optimization/))
\\[ L \;= \frac{||W||^2}{2} \; - \; \sum_{i=1}^n \alpha_i(y_i(W^Tx_i+b)-1)\\]
KKT conditions are
\\[ \frac{\partial L}{\partial W} \; = \; W - \sum_{i=1}^n \alpha_iy_ix_i = 0\\]
\\[\frac{\partial L}{\partial b} \; = \; -\sum_{i=1}^n\alpha_iy_i = 0\\]
\\[ \alpha_i \ge 0 \quad \text{ and } \alpha_i(y_i(W^Tx_i+b)-1) = 0 \\]
so from above two eqations \\( W = \sum_{i=1}^n \alpha_iy_ix_i\\) and \\(\sum_{i=1}^n \alpha_iy_i = 0\\).  
Subtituting these values in Lagrangian L
\\[ L \;=\; = \frac{1}{2}(\sum_{i=1}^n\alpha_iy_ix_i)(\sum_{i=1}^n\alpha_iy_ix_i) - \sum_{i=1}^n\alpha_iy_ix_i.W - \sum_{i=1}^n\alpha_iy_ib + \sum_{i=1}^n\alpha_i\\]
\\[ L \;=\; = \frac{1}{2}(\sum_{i=1}^n\alpha_iy_ix_i)(\sum_{i=1}^n\alpha_iy_ix_i) - (\sum_{i=1}^n\alpha_iy_ix_i)(\sum_{i=1}^n\alpha_iy_ix_i) - \sum_{i=1}^n\alpha_iy_ib + \sum_{i=1}^n\alpha_i\\]
\\[ L \;=\; = -\frac{1}{2}(\sum_{i=1}^n\alpha_iy_ix_i)(\sum_{i=1}^n\alpha_iy_ix_i) - \sum_{i=1}^n\alpha_iy_ib + \sum_{i=1}^n\alpha_i\\]
\\(\sum_{i=1}^n\alpha_iy_ib\\)  will be zero because b is constant and  \\(\sum_{i=1}^n \alpha_iy_i = 0\\).
\\[ L \;=\; = \sum_{i=1}^n\alpha_i -\frac{1}{2}(\sum_{i=1}^n\alpha_iy_ix_i)(\sum_{i=1}^n\alpha_iy_ix_i)\\]
here conditions are \\(\alpha_i \ge 0\\)  and \\(\sum_{i=1}^n \alpha_iy_i = 0\\).  This is called as Dual formulation of SVM. This has many advantages over primal formulation.



