---
layout: post
title: Support Vector Machine
date: 2018-06-07
description: # add
img:  # Add image post (optional)
tags: [SVM,Maximum Margin]
---
SVM is one type of classification and regression algorithm  which is maily focused on the maximal margin hyperplane. Figure below shows two different classes seperating by three hyperplanes and also we can plot infinity hyperplanes to classify those two classes.  
![svm margin]({{site.baseurl}}/assets/img/svm.jpg)  
Although their training errors are zero there is no guarantee that the hyperplanes will perform equally well on unseen data.The classifier must choose one of these hyperplanes to represent its decision boundary, based on how well they are expected to perform on test samples.
Decision boundaries with large margins tend to have better generalization
errors than those with small margins.Intuitively, if the margin is small, then any slight perturbations to the decision boundary can have quite a significant impact on its classification Classifiers that produce decision boundaries with small margins are therefore more susceptible to model overfitting and tend to generalize poorly on previously unseen data, so SVM tries to locate the hyperplane such that distance is maximum from both the classes.  
##### How SVM is selecting margin: 
Consider a binary classification problem as shown in below figure and the hyperplane is \\(\pi\\). and two planes whcich are parallel to the hyperplane and passing through nearest point of each class are  \\(\pi_+\\) and  \\(\pi_-\\) as shown in below fig.  
![svm explanation]({{site.baseurl}}/assets/img/max_margin.jpg)  
Maximizing margin means maximizing distance between the \\(\pi_+\\) and  \\(\pi_-\\).so  our obkective is to maximize the distance between those two planes. Let's assume for all positive points y = +1 and for all negative points y = -1 so \\(y_i(W^Tx+b) \ge 1\\) for all points.To compute the margin, let x1 be a data point located on \\(\pi_+\\) and x2 be a data point on \\(\pi_-\\).
\\[\pi_+ is W^Tx_1 + b = +1\\]
\\[\pi_- is W^Tx_2 + b = -1\\]
by subtracting
\\[W.(x_1-x_2) = 2\\]
\\[||W|| * d = 2\\]
\\[d = \frac{2}{||W||}\\]
so our optimization problem is
\\[ Max \frac{2}{||W||} { Such That } y_i(W^Tx_i+b) \ge 1 { For all i}\\]
Max x = Min 1/x so we can write our problem as  
\\[ \text{Min } \frac{||W||^2}{2} { Such That } y_i(W^Tx_i+b) \ge 1 { For all i}\\]
This SVM formulation presented in the previous section constructs only decision boundaries that are mistake-free. so for non separable data all constraints wont satisfy. This is called Hard margin SVM. so The learning algorithm in SVM must consider the trade-off between
the width of the margin and the number of training errors committed by the decision boundary. For this introducing positive-valued slack variables \\(\xi\\) into the constraints of the optimization such that for all positive points in positive side \\(\xi_i = 0\\), for all negative points in negative side \\(\xi_i = 0\\), for all positive points in negative side \\(\xi_i > 0\\) and \\(\xi_i\\) is distance from \\(\pi_+\\), for all negative points in positive side \(\xi_i > 0\\) and \\(\xi_i\\) is distance from \\(\pi_-\\). \\(\xi_i\\) is high for the points far away from correct hyperplane  \\(\pi_+\\) or \\(\pi_-\\).  
So our new formulation is 
\\[ ( W^* , b^* ) = \text{ arg min } & \frac{||W||^2}{2} { Such  That } & y_i(W^Tx_i+b) \ge 1-\xi_i { For all i and } \xi_i > 0\\]
